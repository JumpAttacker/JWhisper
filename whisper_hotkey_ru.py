import queue
import threading
import time
from typing import List

import numpy as np
import sounddevice as sd
from pynput import keyboard
from faster_whisper import WhisperModel
import pyautogui
import pyperclip

# -------------------- SETTINGS --------------------
SAMPLE_RATE = 16000         # Microphone sample rate
CHANNELS = 1                # Mono
DTYPE = "float32"           # Changed to float32 for better compatibility
PUSH_TO_TALK_KEY = keyboard.Key.f9  # Hold F9 while speaking

# Model: small is fast and good quality; can use "medium" or "large-v3"
WHISPER_MODEL_NAME = "small"   # "tiny" | "base" | "small" | "medium" | "large-v3"
WHISPER_DEVICE = "cpu"         # "cpu" or "cuda" (if NVIDIA GPU available)
WHISPER_COMPUTE_TYPE = "int8"  # "int8"/"int8_float16" (CPU) | "float16"/"int8_float16" (GPU)

LANGUAGE = "ru"                # Force Russian language
MIN_SPEECH_SEC = 0.5           # Increased minimum recording duration
PASTE_WITH_CLIPBOARD = True    # True: paste via clipboard (Ctrl+V), False: type character by character
AUDIO_THRESHOLD = 0.001        # Lowered threshold for better detection

# -------------------- GLOBAL STATE --------------------
recording_flag = False
buffer_lock = threading.Lock()
buffers: List[np.ndarray] = []  # Store audio chunks while key is held

# Initialize model at startup (first run will download weights to cache)
print("Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÐµÐ»Ð¸ Whisper... (Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð·Ð°Ð¿ÑƒÑÐº Ð¼Ð¾Ð¶ÐµÑ‚ Ð·Ð°Ð½ÑÑ‚ÑŒ Ð²Ñ€ÐµÐ¼Ñ)")
model = WhisperModel(WHISPER_MODEL_NAME, device=WHISPER_DEVICE, compute_type=WHISPER_COMPUTE_TYPE)
print(f"ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°: {WHISPER_MODEL_NAME}")

# List available audio devices
print("\nÐ”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð°ÑƒÐ´Ð¸Ð¾ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°:")
devices = sd.query_devices()
for i, device in enumerate(devices):
    if device['max_input_channels'] > 0:
        print(f"  {i}: {device['name']} - {device['max_input_channels']} ÐºÐ°Ð½Ð°Ð»Ð¾Ð²")

default_device = sd.query_devices(kind='input')
print(f"\nÐ˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð¾: {default_device['name']}")
print("=" * 60)

# -------------------- AUDIO STREAM --------------------
def audio_callback(indata, frames, time_info, status):
    global recording_flag, buffers
    if status:
        # Driver overflow/warnings
        print("Audio status:", status)
    if recording_flag:
        # Copy the current fragment
        with buffer_lock:
            buffers.append(indata.copy())

# -------------------- RECORDING LOGIC --------------------
def start_recording():
    global recording_flag, buffers
    with buffer_lock:
        buffers = []
    recording_flag = True
    print("â–¶ Ð—Ð°Ð¿Ð¸ÑÑŒ Ð½Ð°Ñ‡Ð°Ð»Ð°ÑÑŒ (ÑƒÐ´ÐµÑ€Ð¶Ð¸Ð²Ð°Ð¹Ñ‚Ðµ F9)...")

def stop_recording_and_transcribe():
    global recording_flag, buffers
    recording_flag = False
    print("â–  Ð—Ð°Ð¿Ð¸ÑÑŒ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°, Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°ÑŽ...")

    # Concatenate all chunks
    with buffer_lock:
        if not buffers:
            print("ÐŸÑƒÑÑ‚Ð°Ñ Ð·Ð°Ð¿Ð¸ÑÑŒ - Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÑŽ.")
            return
        audio_chunk = np.concatenate(buffers, axis=0)
        buffers = []

    # Flatten if multi-channel
    if len(audio_chunk.shape) > 1:
        audio_chunk = audio_chunk.flatten()
    
    # Check duration
    duration_sec = len(audio_chunk) / SAMPLE_RATE
    if duration_sec < MIN_SPEECH_SEC:
        print(f"Ð¡Ð»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ ({duration_sec:.2f}Ñ) - Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÑŽ.")
        return

    # Check audio level
    audio_level = np.abs(audio_chunk).mean()
    max_level = np.abs(audio_chunk).max()
    print(f"Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð°ÑƒÐ´Ð¸Ð¾: ÑÑ€ÐµÐ´Ð½Ð¸Ð¹={audio_level:.4f}, Ð¼Ð°ÐºÑ={max_level:.4f}, Ð”Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ: {duration_sec:.2f}Ñ")
    
    if audio_level < AUDIO_THRESHOLD:
        print("Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð°ÑƒÐ´Ð¸Ð¾ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð½Ð¸Ð·ÐºÐ¸Ð¹ - Ñ€ÐµÑ‡ÑŒ Ð½Ðµ Ð¾Ð±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð°.")
        print("ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð³Ñ€Ð¾Ð¼Ñ‡Ðµ Ð¸Ð»Ð¸ ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð² Windows.")
        return

    # Normalize audio if levels are low but above threshold
    if max_level < 0.1:
        print("ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¸Ñ…Ð¾Ð³Ð¾ Ð°ÑƒÐ´Ð¸Ð¾...")
        audio_chunk = audio_chunk / max_level * 0.5

    # Audio is already float32 from sounddevice
    audio_float = audio_chunk

    # Transcribe
    try:
        print(f"Ð Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ...")
        segments, info = model.transcribe(
            audio_float,
            language="ru",              # Force Russian
            vad_filter=True,            # Simple VAD to remove silence
            beam_size=5,
            best_of=5,
            without_timestamps=False,
            temperature=0.0,
            compression_ratio_threshold=2.4,
            log_prob_threshold=-1.0,
            no_speech_threshold=0.6,
            initial_prompt="Ð­Ñ‚Ð¾ Ñ€ÑƒÑÑÐºÐ°Ñ Ñ€ÐµÑ‡ÑŒ."  # Hint for better Russian recognition
        )
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ: {e}")
        return

    # Combine segments into one string
    text_parts = []
    for seg in segments:
        text_parts.append(seg.text.strip())
    final_text = " ".join(t for t in text_parts if t).strip()

    if not final_text:
        print("Ð¢ÐµÐºÑÑ‚ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½.")
        print("ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ:")
        print("1. Ð“Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ Ñ‡ÐµÑ‚Ñ‡Ðµ Ð¸ Ð³Ñ€Ð¾Ð¼Ñ‡Ðµ")
        print("2. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð² Windows")
        print("3. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ test_audio.py Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°")
        return

    print(f"ðŸ“„ Ð¢ÐµÐºÑÑ‚: {final_text}")

    # Insert into active window
    try:
        if PASTE_WITH_CLIPBOARD:
            pyperclip.copy(final_text)
            time.sleep(0.05)
            pyautogui.hotkey("ctrl", "v")
        else:
            # Character-by-character typing (slower, but doesn't use clipboard)
            pyautogui.typewrite(final_text)
        print("âœ“ Ð¢ÐµÐºÑÑ‚ Ð²ÑÑ‚Ð°Ð²Ð»ÐµÐ½")
    except Exception as e:
        print("ÐžÑˆÐ¸Ð±ÐºÐ° Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ñ‚ÐµÐºÑÑ‚Ð°:", e)

# -------------------- KEY HANDLING --------------------
pressed_keys = set()

def on_press(key):
    # Start recording when F9 is pressed, if not already recording
    if key == PUSH_TO_TALK_KEY and PUSH_TO_TALK_KEY not in pressed_keys:
        pressed_keys.add(PUSH_TO_TALK_KEY)
        if not recording_flag:
            start_recording()

def on_release(key):
    # Stop when F9 is released
    if key == PUSH_TO_TALK_KEY:
        if PUSH_TO_TALK_KEY in pressed_keys:
            pressed_keys.remove(PUSH_TO_TALK_KEY)
        if recording_flag:
            stop_recording_and_transcribe()
    
    # Exit on ESC key
    if key == keyboard.Key.esc:
        print("\nESC Ð½Ð°Ð¶Ð°Ñ‚ - Ð²Ñ‹Ñ…Ð¾Ð´...")
        return False

# -------------------- MAIN --------------------
def main():
    print("Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! Ð£Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ð¹Ñ‚Ðµ F9, Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ â†’ Ð¾Ñ‚Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ F9, Ð¸ Ñ‚ÐµÐºÑÑ‚ Ð²ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑÑ Ð² Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ðµ Ð¾ÐºÐ½Ð¾.")
    print("ÐšÑƒÑ€ÑÐ¾Ñ€ Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð² Ð½ÑƒÐ¶Ð½Ð¾Ð¼ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¸ (Ð±Ð»Ð¾ÐºÐ½Ð¾Ñ‚, Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð¸ Ñ‚.Ð¿.).")
    print("ÐÐ°Ð¶Ð¼Ð¸Ñ‚Ðµ ESC Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð° Ð¸Ð· Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹.")
    print("=" * 60)
    print("ÐÐÐ¡Ð¢Ð ÐžÐ•ÐÐž Ð”Ð›Ð¯ Ð Ð£Ð¡Ð¡ÐšÐžÐ“Ðž Ð¯Ð—Ð«ÐšÐ")
    print("=" * 60)
    
    # Start audio stream
    try:
        with sd.InputStream(callback=audio_callback, channels=CHANNELS, samplerate=SAMPLE_RATE, dtype=DTYPE):
            with keyboard.Listener(on_press=on_press, on_release=on_release) as listener:
                listener.join()
    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
        print("\nÐ ÐµÑˆÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼:")
        print("1. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°")
        print("2. Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ 'python test_audio.py' Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð°ÑƒÐ´Ð¸Ð¾ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²")
        print("3. Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ñ€Ð¸Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽÑ‚ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½")
        print("4. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð² Windows")

if __name__ == "__main__":
    main()